{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"NOTEBOOK_30-11-2021.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"5LDY7Ozw2JcB"},"source":["# Árboles de decisión."]},{"cell_type":"markdown","metadata":{"id":"A9MphvPH2JcI"},"source":["Un árbol de decisión es un predictor, $h: X \\longrightarrow Y$, que predice la etiqueta asociada con una instancia $x$ viajando desde un nodo raíz de un árbol a una hoja (este es un algoritmo de clasificación, por lo que $Y \\subset \\mathbb{N}$). Para explicar un ejemplo simple, nos centramos en la  clasificación binaria, a saber, $Y = \\{0, 1\\}$, pero los árboles de decisión también se pueden aplicar para otros problemas de predicción con más de dos resultados ($Y$ puede ser un subconjunto de los números naturales). En cada nodo de la ruta de la raíz a la hoja, el hijo sucesor se elige sobre la base de una división del espacio de entrada. Por lo general, la división se basa en una de las características de x (atributo) o en un conjunto predefinido de reglas de división. Una hoja contiene una etiqueta específica. A continuación se pone un ejemplo de un árbol de decisión en el cuál nos dice si una papaya es sabrosa a partir de su color y suavidad (softness)"]},{"cell_type":"markdown","metadata":{"id":"Vb75OQ1_2JcK"},"source":["![title](DT.png)"]},{"cell_type":"markdown","metadata":{"id":"S2_Sx4MS2JcL"},"source":["Para verificar si una papaya dada es sabrosa o no, el árbol de decisión primero examina el color de la papaya. Si este color no está en el rango de verde pálido a amarillo pálido, entonces el árbol predice inmediatamente que la papaya no es sabrosa, sin realizar pruebas adicionales. De lo contrario (si está en el rango de colores), el árbol ahora examina la suavidad de la papaya. Si el nivel de suavidad de la papaya es tal que cede ligeramente la presión de la palma, el árbol de decisiones predice que la papaya es sabrosa. De lo contrario, la predicción es \"no sabrosa\". El ejemplo anterior subraya una de las principales ventajas de los árboles de decisión: el clasificador resultante es muy sencillo de entender e interpretar."]},{"cell_type":"markdown","metadata":{"id":"yOW-9tR22JcL"},"source":["## Escogiendo los atributos de prueba.\n","\n","La búsqueda utilizada en el aprendizaje del árbol de decisión está diseñada para minimizar aproximadamente la profundidad del árbol final. La idea es elegir el atributo que llegue lo más lejos posible para proporcionar una clasificación exacta de los ejemplos. Todo lo que necesitamos, entonces, es una medida formal de \"que tan bueno es\" y \"realmente útil\". Utilizaremos la noción de ***ganancia de entropia de información***, que se define en términos de entropía, la cantidad fundamental en la teoría de la información\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ayhHOwkA2JcM"},"source":["La entropía es una medida de la incertidumbre de una variable aleatoria; la adquisición de información corresponde a una reducción de la entropía. Una variable aleatoria con un solo valor, por ejemplo una moneda en donde siempre sale cara, no tiene incertidumbre y, por lo tanto, su entropía se define como cero; por lo tanto, no obtenemos información al observar su valor. La entropía puede interpretarse también como el número de bits mínimo necesario para codificar la clasificación de cualquier miembro de tu conjunto de datos. Un lanzamiento de una moneda justa en donde es igualmente probable que salga cara o cruz, 0 o 1,  cuenta como \"1 bit\" de entropía. La tirada de un dado de cuatro lados tiene 2 bits de entropía, porque se necesitan dos bits para describir una de las cuatro opciones igualmente probables. Ahora considere una moneda injusta que sale cara el 99% del tiempo. Intuitivamente, esta moneda tiene menos incertidumbre que la moneda justa, si suponemos que las caras se equivocarán solo el 1% del tiempo, por lo que nos gustaría que tenga una medida de entropía cercana a cero, pero positiva. En general, la entropía de una variable aleatoria $V$ con valores $v_k$, cada uno con probabilidad $P(v_k)$, se define como\n","$$H(v) = \\sum_{k}P(v_k)log_2\\frac{1}{P(v_k)} = -\\sum_{k}P(v_k)log_2P(v_k) $$\n","\n","Ahora verifiquemos que la entropía de una moneda justa es 1 (\"1 bit\"):\n","$$H(fair\\, coin) = -0.5log_2(0.5)-0.5log_2(0.5) = 1$$\n","\n","Si la moneda está cargada de forma que el 99% de las veces caiga cara, se tiene:\n","$$H(loaded\\,coin) = -0.99log_2(0.99)-0.01log_2(0.01)\\approx 0.08$$\n","\n","Para el dado de cuatro lados se tiene:\n","$$H(4-dice) = -0.25log_2(0.25)-0.25log_2(0.25)-0.25log_2(0.25)-0.25log_2(0.25) = 2$$\n"]},{"cell_type":"markdown","metadata":{"id":"fpPlWuyX2JcN"},"source":["Ya vimos que es la entropía, ahora falta definir otra cantidad que nos dice la \"ganancia\" de información. Definimos la ganancia $G$ como:\n","$$G(D,A) = H(D)-\\sum_{a_k}\\frac{|S_{a_k}|}{|S|}H(S_{a_k})$$\n","\n","en donde $a_k$ son los valores que puede tomar el atributo $A$, $S_{a_k}$ es el subconjunto de datos que tienen valor $a_k$ en el atributo $A$ y $|S|$ es la cardinalidad del conjunto de datos"]},{"cell_type":"markdown","metadata":{"id":"aUZJ25Sy2JcO"},"source":["# Algoritmo ID3\n","\n","El algoritmo ID3 va seleccionando los atributos que producen mayor ganancia para generar el árbol de decisión."]},{"cell_type":"markdown","metadata":{"id":"E6deKpCF2JcP"},"source":["Veamos un ejemplo de calculo de ganancia y entropía para un conjunto de datos"]},{"cell_type":"markdown","metadata":{"id":"EQ1ETCJq2JcQ"},"source":["| A \t| B \t| C \t| E \t|\n","|---\t|---\t|---\t|---\t|\n","| 0 \t| 1 \t| 1 \t| 1 \t|\n","| 1 \t| 1 \t| 1 \t| 1 \t|\n","| 0 \t| 0 \t| 1 \t| 0 \t|\n","| 1 \t| 0 \t| 0 \t| 1 \t|\n","| 1 \t| 1 \t| 1 \t| 1 \t|"]},{"cell_type":"markdown","metadata":{"id":"Wjn1qCE72JcR"},"source":["Este conjunto tiene los atributos A,B y C, mientras que E son las etiquetas que queremos predecir. Llamemos a este conjunto de datos \"D\". Hagamos los cálculos de entropia y de ganancia para cada atributo.\n"]},{"cell_type":"markdown","metadata":{"id":"uHFyVW0K2JcS"},"source":["$$H(D) = -\\frac{4}{5}log_2(\\frac{4}{5})-\\frac{1}{5}log_2(\\frac{1}{5}) = 0.7219$$"]},{"cell_type":"markdown","metadata":{"id":"Nd2R3KCN2JcS"},"source":["$$G(D,A) = 0.7219 - \\frac{3}{5}H(D_{A_1}) -  \\frac{2}{5}H(D_{A_0})$$"]},{"cell_type":"markdown","metadata":{"id":"i1GfKFiQ2JcS"},"source":["$$G(D,B) = 0.7219 - \\frac{3}{5}H(D_{B_1}) -  \\frac{2}{5}H(D_{B_0})$$"]},{"cell_type":"markdown","metadata":{"id":"FprLNIZh2JcT"},"source":["$$G(D,C) = 0.7219 - \\frac{4}{5}H(D_{C_1}) -  \\frac{1}{5}H(D_{C_0})$$"]},{"cell_type":"markdown","metadata":{"id":"llwE1En02JcT"},"source":["Calculamos las entropías restantes:\n","$$H(D_{A_1}) = - \\frac{3}{3}log_2(\\frac{3}{3}) = 0$$\n","\n","$$H(D_{A_0}) = - \\frac{1}{2}log_2(\\frac{1}{2}) - \\frac{1}{2}log_2(\\frac{1}{2}) = 1$$\n","\n","$$H(D_{B_1}) = - \\frac{3}{3}log_2(\\frac{3}{3}) = 0$$\n","\n","$$H(D_{B_0}) = - \\frac{1}{2}log_2(\\frac{1}{2}) - \\frac{1}{2}log_2(\\frac{1}{2}) = 1$$\n","\n","$$H(D_{C_1}) = -\\frac{3}{4}log_2(\\frac{3}{4}) -\\frac{1}{4}log_2(\\frac{1}{4}) = 0.8113$$ \n","\n","$$H(D_{C_0}) = - \\frac{1}{1}log_2(\\frac{1}{1}) = 0$$\n"]},{"cell_type":"markdown","metadata":{"id":"aTRwqbEd2JcX"},"source":["Las ganancias quedan finalmente como:\n","$$G(D,A) = 0.7219 - \\frac{3}{5}(0) -  \\frac{2}{5}(1) = 0.3219$$"]},{"cell_type":"markdown","metadata":{"id":"40G8LUxj2JcY"},"source":["$$G(D,B) = 0.7219 - \\frac{3}{5}(0) -  \\frac{2}{5}(1) = 0.3219$$"]},{"cell_type":"markdown","metadata":{"id":"qoi66lcy2JcY"},"source":["$$G(D,C) = 0.7219 - \\frac{4}{5}(0.8113) -  \\frac{1}{5}(0) = 0.07286$$"]},{"cell_type":"markdown","metadata":{"id":"tafP25eM2JcY"},"source":["Al tener dos atributos con la misma ganancia se escoge uno de manera aleatoria, ya que tanto A como B tienen la ganancia máxima. Este proceso se hace sucesivamente hasta tomar en cuenta todos los atributos."]},{"cell_type":"markdown","metadata":{"id":"I-mkEmQA2JcY"},"source":["## Prunning (Poda)"]},{"cell_type":"markdown","metadata":{"id":"VN6i04EZ2JcZ"},"source":["El algoritmo ID3 descrito anteriormente todavía tiene un gran problema: el árbol devuelto generalmente será muy grande y también puede haber un sobreajuste en los datos (overfitting). Esos árboles pueden tener un riesgo empírico bajo, pero su verdadero riesgo tenderá a ser alto, tanto de acuerdo con nuestro análisis teórico como en la práctica. Una solución es limitar el número de iteraciones de ID3, lo que lleva a un árbol con un número limitado de nodos. Otra solución común es ***podar*** el árbol después de su construcción, con la esperanza de reducirlo a un árbol mucho más pequeño, pero aún con un error empírico similar.  Por lo general, la poda se realiza mediante una caminata de abajo hacia arriba en el árbol. Cada nodo puede ser reemplazado con uno de sus subárboles o con una hoja."]},{"cell_type":"markdown","metadata":{"id":"EzxTNa7f2JcZ"},"source":["# Utilizando aŕboles de decisión con scikit-learn"]},{"cell_type":"code","metadata":{"id":"iSzBKC9U2Jca"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4xT6oYg2Jcb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXW7qkO42Jcc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0A39fU4P2Jcc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_32sQxDt2Jcc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wmYfpq82Jcc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ud2wZxj2Jcc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Op7n8Avl2Jcd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XTsryRq12Jcd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B1-MLZ8H2Jcd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z5vjknwK2Jcd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4Ybg7dJ2Jcd"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DXXogEKK2Jce"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xq9m1Qne2Jce"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0uaU70x2Jcf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHolFfqO2Jcf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fi-zXvvm2Jcf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYhPvwqy2Jcf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDgwnBar2Jcf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3lpQjGPG2Jcf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lk1gXKI12Jcf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9YhTPUV2Jcf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnxgqhKR2Jcg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tpLZwTH2Jcg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XoegcbMa2Jcg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMPDv0Y-2Jcg"},"source":["# Random forests"]},{"cell_type":"markdown","metadata":{"id":"dvDsXUhr2Jcg"},"source":["Otra forma de reducir el peligro de sobreajuste (overfitting) es mediante la construcción de un conjunto de árboles. En particular, a continuación describimos el método de ***bosques aleatorios (random forests)***."]},{"cell_type":"markdown","metadata":{"id":"3fA4fhCP2Jcg"},"source":["Un bosque aleatorio es un clasificador que consiste en una colección de árboles de decisión, donde cada árbol se construye aplicando un algoritmo A en el conjunto de entrenamiento $S$ y un vector aleatorio adicional, $\\theta$, donde $\\theta$ se muestrea de alguna distribución. La predicción del bosque aleatorio se obtiene por mayoría de votos sobre las predicciones de los árboles individuales."]},{"cell_type":"markdown","metadata":{"id":"Njw0_T7C2Jch"},"source":["Para especificar un bosque aleatorio particular, necesitamos definir el algoritmo $A$ y la distribución sobre $\\theta$. Hay muchas formas de hacer esto y aquí describimos una opción en particular. Generamos $\\theta$ de la siguiente manera. Primero, tomamos una submuestra aleatoria de $S$; a saber, muestreamos un nuevo conjunto de entrenamiento $S^\\prime$ de tamaño $m^\\prime$ usando la distribución uniforme sobre $S$. Segundo, construimos una sucesión $I_1, I_2,\\dots,$ donde cada $I_t$ es un subconjunto de $[d]$ ($d$ es el número de atributos o características del dataset) de tamaño $k$, que se genera mediante el muestreo uniforme de elementos aleatorios de $[d]$. Todas estas variables aleatorias forman el vector $\\theta$. Luego, el algoritmo $A$ desarrolla un árbol de decisión (por ejemplo, usando el algoritmo ID3) basado en la muestra $S^\\prime$, donde en cada etapa de división del algoritmo, el algoritmo se limita a elegir una característica que maximice la ganancia del conjunto $I_t$. Intuitivamente, si $k$ es pequeño, esta restricción puede evitar el sobreajuste.\n"]},{"cell_type":"markdown","metadata":{"id":"EWOzxRyk2Jch"},"source":["## Random forests con scikit-learn"]},{"cell_type":"code","metadata":{"id":"6a7sA6jl2Jch"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RN4FnOXK2Jch"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Wtv8s6w2Jch"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfirQYRz2Jch"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YprXXCVp2Jci"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zSzgJ8s2Jci"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1RQFMiZ82Jci"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3y8Wn9a2Jci"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7g9UsoQ2Jci"},"source":[""],"execution_count":null,"outputs":[]}]}